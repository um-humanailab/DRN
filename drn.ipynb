{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "        n_lower : int, n_upper : int, \n",
    "        q_lower : int, q_upper : int,\n",
    "        weight_init=0.1,\n",
    "        weight_init_method='uniform'\n",
    "    ):\n",
    "        # factory_kwargs = {'device': torch.device, 'dtype': dtype}\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.n_lower = n_lower\n",
    "        self.n_upper = n_upper\n",
    "        self.q_lower = q_lower\n",
    "        self.q_upper = q_upper\n",
    "        self.weight_init = 0.1\n",
    "        self.weight_init_method = weight_init_method\n",
    "\n",
    "        self.D = self.initD(n_lower, q_upper, n_lower, n_upper)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # if self.weight_init_method == 'uniform': # IMPLEMENT LATER\n",
    "        self.weights = nn.Parameter(init.uniform_(torch.empty(self.n_upper, self.n_lower), a=-self.weight_init, b=self.weight_init))\n",
    "        self.ba = nn.Parameter(init.uniform_(torch.empty(self.n_upper, 1), a=-self.weight_init, b=self.weight_init))\n",
    "        self.bq = nn.Parameter(init.uniform_(torch.empty(self.n_upper, 1), a=-self.weight_init, b=self.weight_init))\n",
    "        self.lama = nn.Parameter(init.uniform_(torch.empty(self.n_upper, 1), a=0, b=1))\n",
    "        self.lamq = nn.Parameter(init.uniform_(torch.empty(self.n_upper, 1), a=0, b=1))\n",
    "        # elif self.weight_init_method == 'normal':\n",
    "        # elif self.weight_init_method == 'glorot_normal':\n",
    "    \n",
    "    def initD(self, q_lower, q_upper, n_lower, n_upper):\n",
    "        D_np = np.zeros((q_upper, q_lower))\n",
    "\n",
    "        for s1 in range(q_upper):\n",
    "            for s0 in range(q_lower):\n",
    "                D_np[s1, s0] = np.exp(-((float(s0)/q_lower - float(s1)/q_upper) ** 2)) # suggest for improvement\n",
    "        \n",
    "        Dnp = D_np.reshape((q_upper, q_lower, 1, 1))\n",
    "        D_tensor = torch.tensor(Dnp, dtype=torch.float32)\n",
    "        D = torch.tile(D_tensor, [1, 1, n_upper, n_lower])\n",
    "        return D\n",
    "    \n",
    "    # returns log(exp(B)) which is B\n",
    "    def cal_logexp_bias(self, q):\n",
    "        # each contains multiple nodes bias values, of size nu x 1\n",
    "        s0 = torch.tensor(torch.arange(q).reshape((1, q)))\n",
    "\n",
    "        # need account for multiple nodes in layer\n",
    "        # s0 - b : (1 x q) x (nu x 1) = nu x q\n",
    "        B = -(self.bq * torch.pow(s0 / q - self.lamq, 2) + self.ba * torch.abs(s0 / q - self.lama, 2))\n",
    "        return B\n",
    "\n",
    "    def forward(self, P):\n",
    "        # MIGHT HAVE PROLEMS HERE LATER WITH BATCH SIZE (BE VARY)\n",
    "        Ptile = torch.tile(torch.reshape(P,[-1, 1, self.n_lower, self.q_lower, 1]), [1, self.n_upper, 1, 1, 1])  # bs x nu x nl x ql x 1\n",
    "        T = torch.transpose(torch.pow(self.D, self.weights), [2, 3, 0, 1])  # nu x nl x qu x ql\n",
    "        Pw_unclipped = torch.squeeze(torch.einsum('jklm,ijkmn->ijkln', T, Ptile), axis=[4])   # bs x nu x nl x qu x 1 -> bs x nu x nl x qu\n",
    "         # clip Pw by value to prevent zeros when weight is large\n",
    "        Pw = torch.clamp(Pw_unclipped, 1e-15, 1e+15)\n",
    "        \n",
    "        # perform underflow handling (product of probabilities become small as no. neighbors increase)\n",
    "        # 1. log each term in Pw\n",
    "        logPw = torch.log(Pw)  # bs x nu x nl x qu\n",
    "        # 2. sum over neighbors\n",
    "        logsum = torch.sum(logPw, axis=2)       # bs x nu x qu\n",
    "        # 3. log of exp of bias terms: log(expB) = exponent_B\n",
    "        exponent_B = self.cal_logexp_bias(self.q_upper)  # nu x q\n",
    "        # 4. add B to logsum\n",
    "        logsumB = torch.add(logsum, exponent_B)          # bs x nu x qu\n",
    "        # 5. find max over s0\n",
    "        max_logsum = torch.max(logsumB, axis=2, keep_dims=True)    # bs x nu x qu\n",
    "        # 6. subtract max_logsum and exponentiate (the max term will have a result of exp(0) = 1, preventing underflow)\n",
    "        # Now all terms will have been multiplied by exp(-max)\n",
    "        expm_P = torch.exp(torch.subtract(logsumB, max_logsum))        # bs x nu x qu\n",
    "        # normalize\n",
    "        Z = torch.sum(expm_P, 2, keep_dims=True)\n",
    "        y_normalised = torch.div(expm_P, Z)\n",
    "        \n",
    "        return y_normalised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRN(nn.Module):\n",
    "    def __init__(self, \n",
    "        in_features: int = 1,\n",
    "        num_layers: int = 1,\n",
    "        num_nodes: int = 5,\n",
    "        out_features: int = 1,\n",
    "        q: int = 100, \n",
    "        hidden_q: int = 10\n",
    "    ):\n",
    "        super(DRN, self).__init__()\n",
    "\n",
    "        if num_layers == 0:\n",
    "            self.layer1 = CustomLayer(in_features, out_features, q, q)\n",
    "        else: \n",
    "            self.layer_1 = CustomLayer(in_features, num_nodes, q, hidden_q)\n",
    "            for layer in range (2, num_layers):\n",
    "                setattr(self, f'layer_{layer}', CustomLayer(num_nodes, num_nodes, hidden_q, hidden_q))\n",
    "            self.final_layer = CustomLayer(num_nodes, out_features, hidden_q, q)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        yout = x\n",
    "        for layer in self.children():\n",
    "            yout = layer(yout)\n",
    "        return yout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e21cb67fa4e9bb074a3e521f7910aea634b2c91223142c48e61dc77b2dde3274"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('diffusion': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
