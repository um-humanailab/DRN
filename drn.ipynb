{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=429):\n",
    "    \"\"\"Function to set reproducibility of results\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Frees up unused memory\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    run_id = 0\n",
    "    seed = 0\n",
    "    epoch = 100\n",
    "    learning_rate = 0.1\n",
    "    batch_size = 10\n",
    "    load_W_from_file = False\n",
    "\n",
    "    #NW architecture: Nhidden layers of Nnodes each, ip and op 1 node\n",
    "    # n_in - [n_layers] x n_nodes - n_out\n",
    "    n_in = 1\n",
    "    n_layers = 1\n",
    "    n_nodes = 5\n",
    "    n_out = 1\n",
    "\n",
    "    data_dir = './OU_q100/'\n",
    "    q = 100\n",
    "    hidden_q = 10\n",
    "    Ntrain = 200\n",
    "    Ntest = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "train_x = np.loadtxt(os.path.join(data_dir, 'train_x.dat'))\n",
    "train_y = np.loadtxt(os.path.join(data_dir, 'train_y.dat'))\n",
    "test_x = np.loadtxt(os.path.join(data_dir, 'test_x.dat'))\n",
    "test_y = np.loadtxt(os.path.join(data_dir, 'test_y.dat'))\n",
    "train_x = train_x[:config.Ntrain].reshape((-1, 1, config.q))\n",
    "train_y = train_y[:config.Ntrain].reshape((-1, 1, config.q))\n",
    "test_x = test_x[:config.Ntest].reshape((-1, 1, config.q))\n",
    "test_y = test_y[:config.Ntest].reshape((-1, 1, config.q))\n",
    "\n",
    "class DistDataset(Dataset):\n",
    "    def __init__(self, x_df, y_df):\n",
    "        self.x_df = x_df\n",
    "        self.y_df = y_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = x_df[index]\n",
    "        target = y_df[index] \n",
    "        \n",
    "        return x, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSDivLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JSDivLoss, self).__init__()\n",
    "        self.kl = nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
    "\n",
    "    def forward(self, p: torch.Tensor, q: torch.Tensor):\n",
    "        p, q = p.view(-1, p.size(-1)), q.view(-1, q.size(-1))\n",
    "        m = (0.5 * (p + q)).log()\n",
    "        return 0.5 * (self.kl(m, p.log())) + self.kl(m, q.log())\n",
    "\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "        n_lower : int, n_upper : int, \n",
    "        q_lower : int, q_upper : int,\n",
    "        weight_init=0.1,\n",
    "        weight_init_method='uniform'\n",
    "    ):\n",
    "        # factory_kwargs = {'device': torch.device, 'dtype': dtype}\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.n_lower = n_lower\n",
    "        self.n_upper = n_upper\n",
    "        self.q_lower = q_lower\n",
    "        self.q_upper = q_upper\n",
    "        self.weight_init = weight_init\n",
    "        self.weight_init_method = weight_init_method\n",
    "\n",
    "        self.D = self.initD(n_lower, q_upper, n_lower, n_upper)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # if self.weight_init_method == 'uniform': # IMPLEMENT LATER\n",
    "        self.weights = nn.Parameter(init.uniform_(torch.empty(self.n_upper, self.n_lower), a=-self.weight_init, b=self.weight_init))\n",
    "        self.ba = nn.Parameter(init.uniform_(torch.empty(self.n_upper, 1), a=-self.weight_init, b=self.weight_init))\n",
    "        self.bq = nn.Parameter(init.uniform_(torch.empty(self.n_upper, 1), a=-self.weight_init, b=self.weight_init))\n",
    "        self.lama = nn.Parameter(init.uniform_(torch.empty(self.n_upper, 1), a=0, b=1))\n",
    "        self.lamq = nn.Parameter(init.uniform_(torch.empty(self.n_upper, 1), a=0, b=1))\n",
    "        # elif self.weight_init_method == 'normal':\n",
    "        # elif self.weight_init_method == 'glorot_normal':\n",
    "    \n",
    "    def initD(self, q_lower, q_upper, n_lower, n_upper):\n",
    "        D_np = np.zeros((q_upper, q_lower))\n",
    "\n",
    "        for s1 in range(q_upper):\n",
    "            for s0 in range(q_lower):\n",
    "                D_np[s1, s0] = np.exp(-((float(s0)/q_lower - float(s1)/q_upper) ** 2)) # suggest for improvement\n",
    "        \n",
    "        Dnp = D_np.reshape((q_upper, q_lower, 1, 1))\n",
    "        D_tensor = torch.tensor(Dnp, dtype=torch.float32)\n",
    "        D = torch.tile(D_tensor, [1, 1, n_upper, n_lower])\n",
    "        return D\n",
    "    \n",
    "    # returns log(exp(B)) which is B\n",
    "    def cal_logexp_bias(self, q):\n",
    "        # each contains multiple nodes bias values, of size nu x 1\n",
    "        s0 = torch.tensor(torch.arange(q).reshape((1, q)))\n",
    "\n",
    "        # need account for multiple nodes in layer\n",
    "        # s0 - b : (1 x q) x (nu x 1) = nu x q\n",
    "        B = -(self.bq * torch.pow(s0 / q - self.lamq, 2) + self.ba * torch.abs(s0 / q - self.lama))\n",
    "        return B\n",
    "\n",
    "    def forward(self, P):\n",
    "        # MIGHT HAVE PROLEMS HERE LATER WITH BATCH SIZE (BE VARY)\n",
    "        Ptile = torch.tile(torch.reshape(P,[-1, 1, self.n_lower, self.q_lower, 1]), [1, self.n_upper, 1, 1, 1])  # bs x nu x nl x ql x 1\n",
    "        T = torch.permute(torch.pow(self.D, self.weights), [2, 3, 0, 1])\n",
    "        # T = torch.transpose(torch.pow(self.D, self.weights), [2, 3, 0, 1])  # nu x nl x qu x ql\n",
    "        Pw_unclipped = torch.squeeze(torch.einsum('jklm,ijkmn->ijkln', T, Ptile), dim=4)   # bs x nu x nl x qu x 1 -> bs x nu x nl x qu\n",
    "         # clip Pw by value to prevent zeros when weight is large\n",
    "        Pw = torch.clamp(Pw_unclipped, 1e-15, 1e+15)\n",
    "        \n",
    "        # perform underflow handling (product of probabilities become small as no. neighbors increase)\n",
    "        # 1. log each term in Pw\n",
    "        logPw = torch.log(Pw)  # bs x nu x nl x qu\n",
    "        # 2. sum over neighbors\n",
    "        logsum = torch.sum(logPw, axis=2)       # bs x nu x qu\n",
    "        # 3. log of exp of bias terms: log(expB) = exponent_B\n",
    "        exponent_B = self.cal_logexp_bias(self.q_upper)  # nu x q\n",
    "        # 4. add B to logsum\n",
    "        logsumB = torch.add(logsum, exponent_B)          # bs x nu x qu\n",
    "        # 5. find max over s0\n",
    "        (max_logsum, max_logsum_indices) = torch.max(logsumB, dim=2, keepdim=True)    # bs x nu x qu\n",
    "        # 6. subtract max_logsum and exponentiate (the max term will have a result of exp(0) = 1, preventing underflow)\n",
    "        # Now all terms will have been multiplied by exp(-max)\n",
    "        expm_P = torch.exp(torch.subtract(logsumB, max_logsum))        # bs x nu x qu\n",
    "        # normalize\n",
    "        Z = torch.sum(expm_P, dim=2, keepdim=True)\n",
    "        y_normalised = torch.div(expm_P, Z)\n",
    "        \n",
    "        return y_normalised\n",
    "\n",
    "class DRN(nn.Module):\n",
    "    def __init__(self, \n",
    "        in_features: int = 1,\n",
    "        num_layers: int = 1,\n",
    "        num_nodes: int = 5,\n",
    "        out_features: int = 1,\n",
    "        q: int = 100, \n",
    "        hidden_q: int = 10\n",
    "    ):\n",
    "        super(DRN, self).__init__()\n",
    "\n",
    "        if num_layers == 0:\n",
    "            self.layer1 = CustomLayer(in_features, out_features, q, q)\n",
    "        else: \n",
    "            self.layer_1 = CustomLayer(in_features, num_nodes, q, hidden_q)\n",
    "            for layer in range (2, num_layers+1):\n",
    "                setattr(self, f'layer_{layer}', CustomLayer(num_nodes, num_nodes, hidden_q, hidden_q))\n",
    "            self.final_layer = CustomLayer(num_nodes, out_features, hidden_q, q)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        yout = x\n",
    "        for layer in self.children():\n",
    "            yout = layer(yout)\n",
    "        return yout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "model = DRN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = CustomLayer(1, 5, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15416\\2824450867.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s0 = torch.tensor(torch.arange(q).reshape((1, q)))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): operands do not broadcast with remapped shapes [original->remapped]: [1, 5, 100, 5]->[1, 1, 5, 100, 1, 5] [10, 1, 5, 10, 1]->[10, 1, 5, 1, 1, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(train_x[\u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[39m# print(x)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# torch.reshape(x, [-1, 1, 1, 100, 1])\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# print(x)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# print(x)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# layer(x).shape\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model(x)\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\memosa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 113\u001b[0m, in \u001b[0;36mDRN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    111\u001b[0m yout \u001b[39m=\u001b[39m x\n\u001b[0;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 113\u001b[0m     yout \u001b[39m=\u001b[39m layer(yout)\n\u001b[0;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m yout\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\memosa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 67\u001b[0m, in \u001b[0;36mCustomLayer.forward\u001b[1;34m(self, P)\u001b[0m\n\u001b[0;32m     65\u001b[0m T \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpermute(torch\u001b[39m.\u001b[39mpow(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights), [\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[0;32m     66\u001b[0m \u001b[39m# T = torch.transpose(torch.pow(self.D, self.weights), [2, 3, 0, 1])  # nu x nl x qu x ql\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m Pw_unclipped \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m'\u001b[39;49m\u001b[39mjklm,ijkmn->ijkln\u001b[39;49m\u001b[39m'\u001b[39;49m, T, Ptile), dim\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)   \u001b[39m# bs x nu x nl x qu x 1 -> bs x nu x nl x qu\u001b[39;00m\n\u001b[0;32m     68\u001b[0m  \u001b[39m# clip Pw by value to prevent zeros when weight is large\u001b[39;00m\n\u001b[0;32m     69\u001b[0m Pw \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(Pw_unclipped, \u001b[39m1e-15\u001b[39m, \u001b[39m1e+15\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\memosa\\lib\\site-packages\\torch\\functional.py:360\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[39m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[39m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m--> 360\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): operands do not broadcast with remapped shapes [original->remapped]: [1, 5, 100, 5]->[1, 1, 5, 100, 1, 5] [10, 1, 5, 10, 1]->[10, 1, 5, 1, 1, 10]"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(train_x[0:10])\n",
    "# print(x)\n",
    "# torch.reshape(x, [-1, 1, 1, 100, 1])\n",
    "# print(x)\n",
    "# print(x)\n",
    "# layer(x).shape\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m seed_everything()\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m DRN()\n\u001b[1;32m----> 4\u001b[0m train_set \u001b[39m=\u001b[39m DistDataset(x\u001b[39m=\u001b[39;49mtrain_x, y\u001b[39m=\u001b[39;49mtrain_y)\n\u001b[0;32m      5\u001b[0m val_set \u001b[39m=\u001b[39m DistDataset(x\u001b[39m=\u001b[39mtest_x, y\u001b[39m=\u001b[39mtest_y)\n\u001b[0;32m      7\u001b[0m seed_everything()\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'x'"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "model = DRN()\n",
    "\n",
    "train_set = DistDataset(x=train_x, y=train_y)\n",
    "val_set = DistDataset(x=test_x, y=test_y)\n",
    "\n",
    "seed_everything()\n",
    "training_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=config.batch_size,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=config.batch_size,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "seed_everything()\n",
    "mse_loss = nn.MSELoss()\n",
    "jds_loss = JSDivLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, training_loader, loss_fn, optimizer, Config):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    pbar = tqdm(enumerate(training_loader), total=len(training_loader))\n",
    "    for i, (x, target) in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss = loss.item()\n",
    "        description = f\"Train Loss: {running_loss}\"\n",
    "        pbar.set_descriptio(description)\n",
    "\n",
    "        clear_memory()\n",
    "\n",
    "\n",
    "# def validate():\n",
    "#     model.eval()\n",
    "#     running_loss = 0\n",
    "#     pbar = tqdm(enumerate(training_loader), total=len(training_loader))\n",
    "#     for i, (x, target) in pbar:\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         output = model(x)\n",
    "\n",
    "#         loss = loss_fn(output, target)\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss = loss.item()\n",
    "#         description = f\"Train Loss: {running_loss}\"\n",
    "#         pbar.set_descriptio(description)\n",
    "\n",
    "#         clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 100])\n",
      "torch.Size([10, 1, 1, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create the input tensor\n",
    "input_tensor = torch.randn(10, 1, 100)\n",
    "\n",
    "# Reshape the tensor\n",
    "reshaped_tensor = torch.reshape(input_tensor, (-1, 1, 1, 100, 1))\n",
    "print(input_tensor.shape)\n",
    "print(reshaped_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0483e+00, -7.0600e-02,  8.2982e-01, -2.7604e-01,  6.5104e-01,\n",
      "          -1.0764e+00,  1.1277e+00, -1.2005e+00,  2.1778e-01,  6.5981e-02,\n",
      "           2.6278e-01, -4.3101e-01,  1.9239e+00, -1.0497e+00,  6.3626e-01,\n",
      "           1.0500e+00, -1.1182e-01, -2.7996e-01, -1.3441e+00, -8.7905e-01,\n",
      "           2.6070e-02,  1.3441e-01,  4.2098e-01,  2.3166e-01, -1.3429e+00,\n",
      "          -9.5078e-01, -2.2974e+00,  2.3790e-01, -1.4965e+00,  9.6083e-02,\n",
      "          -4.2727e-01, -2.9522e+00,  5.6465e-01, -7.5040e-01, -1.5394e+00,\n",
      "           2.1988e-01,  3.7527e-02, -8.9407e-02, -6.1276e-01,  1.0504e+00,\n",
      "          -7.7325e-01, -7.0889e-01, -2.6448e-01,  3.3139e-01, -1.2745e+00,\n",
      "          -7.4332e-01,  4.8268e-01, -3.2979e-01, -1.6409e+00,  3.3546e-02,\n",
      "           1.5255e+00,  7.2267e-01, -7.5387e-01,  1.3723e+00, -1.0036e+00,\n",
      "           2.2944e-01,  9.8739e-01,  1.0094e+00, -1.9446e+00, -2.0813e+00,\n",
      "           3.8591e-02,  3.4954e-01,  6.8550e-01, -1.9231e-01,  6.9675e-01,\n",
      "           8.5388e-01,  5.6233e-01, -2.3520e-01,  1.3756e-01,  1.6466e+00,\n",
      "           5.1162e-02,  9.1970e-01, -4.7819e-02,  8.3528e-01,  1.5669e-02,\n",
      "          -7.0519e-01,  1.6533e+00,  1.0745e-01, -1.0236e-01, -8.7989e-01,\n",
      "           8.9534e-01, -2.2145e-01,  2.3094e+00, -1.7552e+00, -3.8676e-01,\n",
      "          -1.4758e-01,  6.3413e-01,  9.8155e-01,  2.0019e-01,  6.5086e-01,\n",
      "          -3.4275e-01,  8.7580e-01,  1.3668e+00, -3.2865e-01, -1.3352e+00,\n",
      "           1.7109e+00, -9.3395e-01,  2.0889e-01,  8.3645e-01,  6.3420e-01]],\n",
      "\n",
      "        [[ 9.9504e-01, -3.0276e-01, -1.3020e+00, -1.3207e+00,  1.9351e+00,\n",
      "          -3.7019e-01, -5.9898e-01,  1.7107e+00,  5.8092e-01, -9.4315e-01,\n",
      "           4.2628e-01,  9.9292e-01, -8.9136e-01, -4.6475e-01,  7.2026e-02,\n",
      "          -2.6840e-01, -4.1095e-01, -9.1257e-01,  6.8131e-01, -1.8844e+00,\n",
      "          -2.2071e+00,  9.7616e-02, -8.7050e-01, -7.0342e-01, -2.7797e+00,\n",
      "          -6.0014e-01, -1.5860e+00,  1.7258e-02,  3.9896e-01,  3.7735e-01,\n",
      "          -8.2186e-01,  7.5080e-01,  5.4721e-01, -2.2036e-01, -7.0317e-01,\n",
      "           1.6023e+00, -6.0688e-01, -1.2819e+00,  5.5387e-01,  4.8487e-01,\n",
      "           9.1080e-01, -2.1867e-01,  1.5412e+00, -1.0066e+00,  4.3355e-01,\n",
      "           1.2758e+00, -5.5118e-01, -3.7846e-01,  1.0648e+00, -1.2871e-01,\n",
      "           4.3316e-01,  8.1295e-01, -1.2312e+00,  9.5532e-01, -1.5255e-01,\n",
      "           2.7861e+00, -1.0419e+00, -5.3479e-01, -5.3642e-01, -2.6046e-01,\n",
      "          -7.5395e-01,  8.0599e-01, -1.3791e+00, -8.2776e-01,  4.9100e-01,\n",
      "          -5.4956e-01,  2.0061e+00,  9.4259e-01,  4.7763e-01, -1.6107e+00,\n",
      "           9.1235e-01, -5.3997e-01, -1.8067e+00, -8.3017e-01, -6.7648e-01,\n",
      "          -4.3211e-01, -7.8475e-01, -4.0185e-01, -4.1110e-01, -3.6686e-01,\n",
      "          -2.2921e+00,  8.1729e-01,  3.9388e-02, -2.1214e-01,  1.3733e+00,\n",
      "          -2.0540e-01, -2.2035e+00, -7.4179e-01, -8.2819e-01, -2.8590e-01,\n",
      "          -2.6537e-01, -7.5617e-01,  5.3703e-01,  2.4439e+00, -4.9696e-01,\n",
      "           8.4808e-01, -1.3029e+00, -2.8487e-01,  6.4262e-02, -2.4411e-01]],\n",
      "\n",
      "        [[-6.3987e-01, -1.2133e+00,  1.0858e+00, -9.7537e-01, -1.2896e+00,\n",
      "          -3.1111e-01,  2.6669e+00, -5.4353e-01,  9.0100e-01, -1.2692e+00,\n",
      "           6.2841e-03, -1.3279e+00, -5.9063e-01,  1.1786e+00,  1.1783e-01,\n",
      "           5.2868e-01, -1.4553e-01, -3.6539e-01,  5.4745e-01, -9.5009e-02,\n",
      "          -2.3040e+00,  8.7468e-01, -9.5581e-01,  6.6777e-01,  4.5853e-01,\n",
      "          -2.4589e+00,  3.9396e-01, -1.1036e+00, -9.8702e-01, -1.0998e+00,\n",
      "          -3.7129e-01, -6.1732e-01, -1.4263e+00, -5.0267e-01, -1.4267e+00,\n",
      "           8.5667e-01,  1.8949e+00, -9.7576e-01, -2.4031e-02, -1.2646e+00,\n",
      "           8.5319e-01,  6.1858e-03,  6.9251e-01,  2.9940e-01,  6.5523e-01,\n",
      "           4.4246e-01,  1.6052e+00, -2.5553e+00, -5.6187e-01, -5.7133e-01,\n",
      "          -7.2671e-01, -2.6967e-01,  3.0084e-02,  3.2338e-01,  1.2603e+00,\n",
      "           1.4958e+00, -9.0979e-01,  1.3900e+00,  3.4114e-01,  4.6770e-01,\n",
      "          -1.3273e+00,  1.4348e+00,  1.9645e+00, -4.2175e-01,  5.3094e-01,\n",
      "          -3.4829e-02,  9.3310e-02,  3.8327e-01,  4.6557e-01,  7.2033e-01,\n",
      "          -1.1910e+00,  3.5722e-02,  6.2005e-02, -9.7596e-01, -1.0262e+00,\n",
      "           9.3274e-01, -3.1487e-01, -1.0536e+00, -1.3146e+00, -7.2108e-01,\n",
      "           3.9074e-01,  4.1900e-01,  2.8038e-01,  5.8059e-01,  5.4181e-01,\n",
      "           1.2115e+00,  1.6559e+00,  4.8489e-01,  5.9367e-02,  2.7686e-01,\n",
      "          -3.3127e-01,  5.0371e-01, -6.6020e-01, -2.3140e-01, -1.3345e-01,\n",
      "           6.0253e-01, -2.3840e+00,  3.2523e-01,  2.0340e+00,  1.1636e+00]],\n",
      "\n",
      "        [[ 9.3178e-02, -1.5927e+00,  2.7492e-01,  2.5024e+00,  5.1385e-01,\n",
      "           2.8683e-01,  9.5899e-02, -1.4863e+00,  1.0680e+00, -1.2659e+00,\n",
      "           1.3172e-01,  5.9784e-02,  1.1844e+00,  1.7852e+00,  9.5614e-01,\n",
      "           1.0915e+00,  6.3888e-01, -1.6261e+00,  8.1406e-01, -1.8002e+00,\n",
      "           5.4731e-01, -5.4644e-01, -1.5388e+00, -3.4725e-01, -1.3673e+00,\n",
      "          -3.2106e-01,  2.8918e-01,  8.8267e-01,  1.7736e+00, -1.4295e+00,\n",
      "           4.9648e-02,  3.0134e-01,  3.8592e-02,  1.8705e+00,  9.2863e-01,\n",
      "           2.7037e-01,  3.1502e-01,  4.9718e-01, -1.1340e-01, -1.6971e-01,\n",
      "           1.3273e+00, -1.2927e-01,  1.0166e+00, -7.4479e-01, -1.9701e-01,\n",
      "          -2.7602e-01, -1.1386e+00,  8.8415e-01,  8.0176e-01,  5.7108e-01,\n",
      "           1.0970e+00, -4.4433e-01, -7.1303e-01,  1.0664e+00,  1.7769e+00,\n",
      "          -4.4395e-01, -2.2620e+00, -2.0023e-01,  3.8745e-01,  2.0252e-01,\n",
      "           1.4550e+00, -4.8235e-01,  1.4339e+00, -8.9580e-01,  4.0612e-02,\n",
      "           5.9294e-01,  1.1024e+00,  1.3262e+00, -1.3296e-01,  1.9137e-01,\n",
      "          -4.4550e-01, -5.5446e-01, -5.8730e-01, -1.5342e+00,  1.5393e+00,\n",
      "           4.9831e-01, -2.6008e-02, -1.1349e+00, -4.2981e-01,  9.1540e-01,\n",
      "           1.4395e-01, -4.9314e-01,  4.0828e-01, -1.7231e+00, -9.8666e-01,\n",
      "          -2.5914e+00,  1.5184e+00,  7.3226e-01,  6.3603e-01,  6.7514e-01,\n",
      "          -4.7434e-01,  6.1323e-01,  1.4467e+00,  4.9824e-01, -1.1301e+00,\n",
      "           7.9616e-01, -7.2968e-01,  1.7748e+00, -6.4126e-01, -5.2529e-02]],\n",
      "\n",
      "        [[-3.0064e+00, -4.9229e-01,  8.8103e-02, -8.4858e-01,  7.9779e-01,\n",
      "          -2.8047e-01,  2.2151e-01,  5.5846e-01,  1.3724e-01, -1.1644e+00,\n",
      "          -2.4361e+00, -7.4273e-01, -5.2869e-01,  1.5296e+00, -1.9499e-01,\n",
      "          -1.2047e+00,  5.3759e-01,  1.3627e+00, -4.1395e-01,  1.9336e-01,\n",
      "           1.6009e+00,  4.2359e-01, -6.3638e-01, -1.7024e+00, -1.6401e+00,\n",
      "           2.0759e-01,  1.9882e-01,  1.2510e+00, -9.1308e-01,  1.5537e+00,\n",
      "           8.4377e-01,  1.4780e+00,  4.8390e-01,  1.5200e+00,  1.2954e+00,\n",
      "           8.2618e-01, -1.4139e+00, -2.6294e-02, -1.3731e+00,  2.7997e-01,\n",
      "          -2.5333e+00, -2.2548e+00,  2.0779e-02, -1.0776e+00, -1.4892e+00,\n",
      "           1.6892e+00, -2.3870e+00, -2.0949e-01, -3.2433e-01, -1.2651e+00,\n",
      "          -2.0957e-01, -2.1441e-01,  5.5532e-01,  6.5184e-01, -4.1280e-01,\n",
      "          -1.1051e+00, -3.9470e-02, -2.3179e-01,  1.1619e-01,  5.3652e-02,\n",
      "          -3.4837e-01,  3.9663e-02, -1.8854e-01, -1.3625e+00, -3.4544e-01,\n",
      "           1.0734e+00,  1.3149e+00, -1.2164e+00,  2.8888e-01,  2.5615e-01,\n",
      "           8.3023e-02,  5.3482e-01, -1.1279e+00,  7.7703e-01, -8.4792e-01,\n",
      "          -5.0718e-01, -6.0083e-01,  1.9117e+00, -1.6381e+00,  4.5643e-01,\n",
      "          -8.8076e-01,  5.4406e-01,  9.0267e-01, -1.3262e+00,  1.6787e+00,\n",
      "          -3.6977e-01, -4.9340e-01,  3.5665e-01, -5.0163e-01,  1.0943e-01,\n",
      "          -9.4894e-01,  1.7913e-01,  1.3601e-02, -1.8411e-01,  1.1561e+00,\n",
      "          -1.4514e+00, -1.2826e+00,  1.5170e+00,  8.0507e-01, -8.7050e-01]],\n",
      "\n",
      "        [[-1.9681e+00, -1.0511e+00, -6.5033e-01, -5.2621e-01,  3.3884e-01,\n",
      "          -3.3758e-01,  2.6407e-01, -2.0241e+00, -5.2177e-01,  2.1439e-01,\n",
      "          -1.2573e+00, -5.5830e-01,  2.9915e-01, -6.3109e-01,  6.2537e-01,\n",
      "          -4.4273e-01,  9.4579e-01,  4.4662e-01,  7.2007e-01,  2.3056e-01,\n",
      "           1.5539e+00,  2.2481e-01,  1.0938e+00, -1.3250e+00,  3.1338e-02,\n",
      "          -1.8569e-01,  1.3285e+00, -8.2145e-01, -3.5482e-01,  2.8989e-01,\n",
      "          -2.2072e+00,  2.7174e-01,  7.3932e-01,  7.4912e-01, -1.3959e+00,\n",
      "           1.0666e+00, -1.5066e+00, -1.2600e-01, -4.7641e-02, -3.0265e-01,\n",
      "          -1.4729e+00,  8.8979e-01, -1.9425e+00, -3.2332e-01,  1.0277e+00,\n",
      "          -6.7061e-01, -1.4273e+00,  3.8550e-01,  2.7668e-01, -1.5388e+00,\n",
      "          -6.3550e-01,  5.2514e-01, -1.1078e+00, -2.8370e-01,  7.8011e-01,\n",
      "          -6.8725e-01, -4.1780e-01,  4.9729e-01, -1.7346e-02,  8.6972e-01,\n",
      "          -5.5824e-01, -1.4502e-01, -1.2450e+00, -4.3157e-02, -9.7442e-01,\n",
      "           1.3096e+00, -7.7306e-01,  3.4462e-01, -3.2867e+00,  2.1314e+00,\n",
      "           2.5750e-01, -2.7717e-01, -9.6424e-01,  7.1176e-01, -8.2104e-01,\n",
      "          -6.8958e-01,  1.1235e+00, -2.1191e-01, -1.3481e-01, -8.9586e-01,\n",
      "          -5.0656e-01, -1.2361e+00,  2.7582e-02,  1.3692e+00, -3.3936e-01,\n",
      "          -2.4184e-01,  8.9537e-01,  8.7368e-01,  4.1317e-01,  6.8859e-01,\n",
      "           2.0920e+00, -9.9433e-01,  7.7551e-01,  1.8249e+00,  3.8697e-01,\n",
      "          -4.2062e-01, -1.2538e+00, -8.2834e-01,  6.6148e-01,  4.1257e-01]],\n",
      "\n",
      "        [[-1.5785e-01,  2.7946e+00, -4.1926e-01, -1.2780e+00,  5.4768e-01,\n",
      "           1.0258e+00,  2.4976e-01, -2.8864e-03,  5.9603e-01,  2.2492e-01,\n",
      "          -8.9884e-01,  1.3026e+00, -7.2098e-01,  2.6151e-01,  1.0021e+00,\n",
      "          -2.2779e-01,  1.3957e+00, -1.6630e+00, -9.8885e-01,  5.2212e-01,\n",
      "           3.2919e-01, -6.4293e-01,  5.8566e-01,  2.4218e-01,  6.5311e-01,\n",
      "          -4.9499e-01,  1.2282e-02,  3.5433e-01, -1.4092e+00, -1.9103e+00,\n",
      "           1.4303e+00,  1.7501e+00, -2.4456e-01,  1.0442e+00, -4.4823e-01,\n",
      "          -1.6836e-01, -7.4391e-01, -1.2264e+00,  3.1365e-01, -3.9142e-02,\n",
      "          -1.1382e-02, -8.7066e-01, -6.8183e-01,  4.9571e-01, -1.0144e+00,\n",
      "           3.8929e-01, -1.0786e+00,  6.8198e-01, -7.0319e-02,  1.1340e+00,\n",
      "           1.4494e+00,  6.9786e-01,  5.1117e-01,  5.7207e-01,  1.0188e+00,\n",
      "           6.5543e-01,  4.5002e-01, -1.0653e+00, -8.5710e-01,  8.0931e-01,\n",
      "           3.4192e-02, -1.0775e+00,  1.1043e+00, -1.7274e+00, -1.9044e+00,\n",
      "          -1.3700e+00,  1.4946e-01, -2.1288e+00, -5.5792e-01, -1.2071e+00,\n",
      "          -1.4827e+00, -1.3852e+00,  1.6998e-01, -9.7108e-01, -1.3531e-01,\n",
      "           7.9902e-01, -1.6428e+00, -7.1615e-01,  2.2212e+00, -1.0241e+00,\n",
      "           1.0282e+00,  5.4366e-03,  3.8204e-01,  8.4049e-02,  1.5643e+00,\n",
      "          -8.2433e-02, -3.8213e-01,  7.9250e-01, -1.3343e+00, -1.4569e-01,\n",
      "          -4.6561e-01,  2.5295e+00, -1.4094e+00, -4.7951e-01, -1.0329e+00,\n",
      "          -7.6787e-01,  1.2361e+00,  8.9653e-01, -6.6949e-01,  6.3382e-01]],\n",
      "\n",
      "        [[-3.1949e-01, -1.8943e-01, -6.2947e-01,  2.4880e+00,  1.1845e+00,\n",
      "          -2.6695e-01,  3.0857e-01, -1.3926e+00,  7.4901e-01,  4.2088e-01,\n",
      "          -8.8778e-01,  5.6508e-01, -7.4212e-01, -4.3228e-01, -8.1901e-01,\n",
      "           2.5940e-01, -1.4961e+00, -1.3728e-01,  2.3876e-01, -5.8923e-02,\n",
      "          -2.9863e-01, -1.6246e+00,  6.3483e-01,  1.7663e-01, -2.7756e-02,\n",
      "           1.4492e+00,  1.2813e+00,  6.8713e-01, -1.0193e-01,  1.9271e-01,\n",
      "           3.2843e-01,  7.5304e-01, -1.0845e+00, -6.0990e-01,  5.3224e-02,\n",
      "           9.1170e-01, -1.1348e-01, -5.6796e-01, -4.1621e-02, -4.0029e-01,\n",
      "           7.7845e-01,  7.8192e-01, -2.5443e+00,  5.3485e-01,  9.4005e-01,\n",
      "           9.5039e-01, -5.2219e-02,  5.0089e-01, -6.0038e-01, -2.3588e-01,\n",
      "          -4.0880e-01, -2.7462e-01,  8.6581e-01,  7.0863e-01, -1.1111e+00,\n",
      "           1.4023e+00,  1.9766e-01,  1.5786e+00, -5.4701e-01,  2.0886e-01,\n",
      "           1.2277e+00,  1.9265e-02, -4.6483e-01, -1.0934e+00, -1.7334e-01,\n",
      "          -6.2183e-01,  1.2147e+00, -6.9933e-01, -2.8757e-02, -1.6208e+00,\n",
      "          -6.7089e-02,  6.6147e-01,  2.2660e-01,  4.8503e-01,  5.0437e-01,\n",
      "          -7.2446e-01,  7.7606e-01, -3.5865e-01, -1.4022e+00,  5.8452e-01,\n",
      "           1.4417e-01, -1.8114e+00, -8.9572e-01,  5.4364e-01, -5.3869e-01,\n",
      "          -7.0700e-01, -1.5023e+00, -1.6772e+00, -8.5347e-01,  1.1141e+00,\n",
      "          -2.7400e+00, -8.4092e-01, -1.5574e+00, -1.0966e+00, -1.9979e+00,\n",
      "          -1.3775e+00,  1.6250e+00,  2.1970e+00,  1.0841e-01,  7.2265e-01]],\n",
      "\n",
      "        [[ 7.1530e-01, -9.0081e-01,  2.5948e-01, -3.6620e-01,  4.3342e-01,\n",
      "          -1.0035e+00, -1.4475e+00,  1.2011e-01,  2.3230e-01,  5.0358e-01,\n",
      "          -1.7170e-01,  2.3167e-01,  6.1171e-02,  5.0710e-01,  1.1210e+00,\n",
      "           1.1149e+00,  1.6992e+00,  6.3678e-01,  3.3274e-01,  1.0905e+00,\n",
      "          -8.3548e-01, -2.0251e+00, -1.3097e-01, -2.0800e-01, -1.1479e+00,\n",
      "           5.5771e-01,  1.2054e+00,  3.3806e-01, -1.2162e+00, -1.6551e-01,\n",
      "          -9.9062e-01, -1.3108e+00,  5.3209e-01, -3.3670e-01,  1.4179e+00,\n",
      "          -3.3457e-01, -4.9611e-01, -6.4539e-01, -9.6441e-01, -1.1883e+00,\n",
      "           1.1261e-02,  6.2418e-02,  7.3390e-01,  1.1075e+00,  8.9263e-01,\n",
      "           2.3528e+00,  3.4303e-01, -1.0521e+00,  5.3043e-01, -8.0963e-01,\n",
      "          -1.4988e+00, -1.7472e-01,  4.6229e-01, -3.0447e-01,  8.2179e-01,\n",
      "           5.2171e-01,  4.4414e-01, -7.2564e-02,  2.2466e+00,  1.2337e+00,\n",
      "           6.9656e-01, -6.7821e-01,  3.4590e-01,  3.0115e-01,  1.6878e-01,\n",
      "          -3.0540e-01,  2.0448e-01, -1.3925e+00,  1.3928e+00,  8.2215e-01,\n",
      "           8.3994e-01, -5.5953e-01, -7.9017e-02,  1.8631e-01, -1.8021e+00,\n",
      "           8.8735e-01,  3.7296e-01, -5.4848e-02,  2.3452e-01,  9.0355e-02,\n",
      "          -8.6371e-01, -2.3870e-01, -6.5437e-01, -2.1483e-01,  4.9862e-01,\n",
      "          -1.4093e-01,  9.1231e-01, -6.4467e-01, -1.8252e+00,  4.4223e-01,\n",
      "          -1.0132e+00,  5.5221e-01, -1.5211e+00, -3.8671e-01, -7.2458e-01,\n",
      "           5.8031e-01,  2.1059e+00, -3.6043e-01,  1.8718e-01,  6.1658e-02]],\n",
      "\n",
      "        [[ 2.1050e+00, -1.5169e+00, -1.1879e+00, -4.2022e-01, -7.7080e-01,\n",
      "          -1.5045e+00,  7.1756e-01,  1.2783e-01,  2.2920e-01, -1.4570e+00,\n",
      "          -1.1713e+00, -6.7415e-01,  2.4345e-01, -9.0114e-01,  1.1310e+00,\n",
      "           1.7259e-01,  4.2568e-01, -5.2530e-01,  5.7065e-01,  6.4960e-01,\n",
      "           1.6625e+00,  5.0458e-01, -1.1052e+00,  4.1379e-01, -2.9926e-01,\n",
      "           5.0970e-01, -8.1042e-01,  9.4190e-01,  1.5588e+00, -1.8550e+00,\n",
      "          -3.5689e-01, -1.1491e+00, -6.2392e-01,  2.0710e-02,  1.2800e+00,\n",
      "          -9.9049e-01, -4.6752e-01, -4.9235e-01,  8.7969e-01, -2.5220e-01,\n",
      "          -1.0357e+00, -9.0559e-01,  5.1986e-01, -4.5310e-01,  6.7358e-02,\n",
      "          -1.4509e+00, -1.0167e-02,  1.8506e-01,  1.5372e-01,  6.8353e-01,\n",
      "          -1.3518e+00, -1.1775e+00, -5.2842e-01, -1.5049e+00, -6.2375e-01,\n",
      "          -1.9115e+00, -2.8186e-01,  1.0843e+00, -1.3697e+00, -7.0830e-01,\n",
      "          -1.7975e-01, -1.8552e-01, -1.1685e-01, -6.5571e-01,  8.5178e-01,\n",
      "           4.8218e-02, -5.7926e-01, -4.8049e-02, -1.0311e-02, -7.2855e-01,\n",
      "           1.2737e+00,  2.5943e-01,  5.9016e-01,  4.5013e-01, -1.9319e+00,\n",
      "           1.6200e+00,  1.1332e+00,  4.2771e-01,  1.4357e+00, -6.2445e-01,\n",
      "          -1.0102e+00, -1.2970e+00,  5.7751e-01, -6.0128e-01, -6.8060e-02,\n",
      "          -7.9074e-01,  1.0054e+00,  1.2641e+00, -9.5852e-01,  2.8750e-01,\n",
      "           5.7423e-01, -2.2223e-01,  5.4624e-01,  6.4030e-01, -1.1129e-01,\n",
      "           5.4825e-01, -5.5845e-01, -1.0764e+00,  1.2873e+00, -8.8168e-01]]])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 5, 1]) torch.Size([5, 1])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transpose() received an invalid combination of arguments - got (Tensor, list), but expected one of:\n * (Tensor input, int dim0, int dim1)\n * (Tensor input, name dim0, name dim1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(train_x[\u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[39m# print(x)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# torch.reshape(x, [-1, 1, 1, 100, 1])\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# print(x)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# print(x)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model(x)\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\memosa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[13], line 113\u001b[0m, in \u001b[0;36mDRN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    111\u001b[0m yout \u001b[39m=\u001b[39m x\n\u001b[0;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 113\u001b[0m     yout \u001b[39m=\u001b[39m layer(yout)\n\u001b[0;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m yout\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\memosa\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[13], line 66\u001b[0m, in \u001b[0;36mCustomLayer.forward\u001b[1;34m(self, P)\u001b[0m\n\u001b[0;32m     64\u001b[0m Ptile \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtile(torch\u001b[39m.\u001b[39mreshape(P,[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_lower, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_lower, \u001b[39m1\u001b[39m]), [\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_upper, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m])  \u001b[39m# bs x nu x nl x ql x 1\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD\u001b[39m.\u001b[39mshape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 66\u001b[0m T \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtranspose(torch\u001b[39m.\u001b[39;49mpow(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mD, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights), [\u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m])  \u001b[39m# nu x nl x qu x ql\u001b[39;00m\n\u001b[0;32m     67\u001b[0m Pw_unclipped \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mjklm,ijkmn->ijkln\u001b[39m\u001b[39m'\u001b[39m, T, Ptile), axis\u001b[39m=\u001b[39m[\u001b[39m4\u001b[39m])   \u001b[39m# bs x nu x nl x qu x 1 -> bs x nu x nl x qu\u001b[39;00m\n\u001b[0;32m     68\u001b[0m  \u001b[39m# clip Pw by value to prevent zeros when weight is large\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (Tensor, list), but expected one of:\n * (Tensor input, int dim0, int dim1)\n * (Tensor input, name dim0, name dim1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config.epoch):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    print(config.learning_rate)\n",
    "    train(epoch, model, training_loader, loss_fn, optimizer, Config)\n",
    "    validate(epoch, model, validation_loader, loss_fn, Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m D \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor((qu, ql, nu, nl))\n\u001b[0;32m      3\u001b[0m W \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor((nu, nl))\n\u001b[1;32m----> 4\u001b[0m torch\u001b[39m.\u001b[39;49mpow(D, W)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "qu, ql, nu, nl = 2, 4, 3, 6\n",
    "D = torch.tensor((qu, ql, nu, nl))\n",
    "W = torch.tensor((nu, nl))\n",
    "torch.pow(D, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39mtranspose(torch\u001b[39m.\u001b[39;49mpow(D, W), [\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "torch.transpose(torch.pow(D, W), [2, 3, 0, 1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e21cb67fa4e9bb074a3e521f7910aea634b2c91223142c48e61dc77b2dde3274"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('diffusion': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
